# Emergent Facial Detection Neural Network Model
Using the Go programming language and Emergent neural simulation software, I replicated the human nervous system's ability to achieve facial detection. 
 The layers of our network correspond to the V1, V4, and Inferotemporal Cortex (IT) regions of the human brain. The model was trained and tested on a dataset of images containing 600+ faces and non-face images, and the results showed that it was able to accurately detect faces with an accuracy of 0.7174.
 The results demonstrate the potential of using Go and Emergent to create neural network models that can accurately detect faces from images. Future steps include refining the edge-detection capabilities to detect facial features from images with varying orientations of direction for more robust detection and recognition capabilities.
 
![Image 1](https://user-images.githubusercontent.com/89750442/210627004-bb896abd-080a-477d-b15f-dda8823b45a5.png)

 
 The model we chose as a template for our purposes is the Computational Cognitive Neuroscience Textbook Object Recognition simulation, due to its gradual ability to perform spatially invariant object recognition over multiple levels of processing. Its generalization results demonstrate that the hierarchical series of representations can operate effectively on varied inputs as long as there exists some similarity among inputs (O’Reilly, 2020).
For our purpose, the model's ability to not only recognize features but to represent them is very important. We hope to mirror the ventral stream's ability to extract the eyes, nose, lips, and overall facial shape so it may be accurately trained and detect faces with new images, and this model's ability to extract objects features is ideal.

After implementing the V1RF simulation's ability to take in .jpg images as inputs, we manipulated the V1 layer size so that it may correspond directly to the pixels of an image. The V1 layer is 100x100 dimensions, corresponding to 10,000 units. Each unit directly corresponds to a pixel of a grayscale training or test image, which also is 100x100 pixels. Each pixel value translates to this layer. 
The grid structure of the model is also highly important, each 10x10 square corresponds to an element of the overall grid, where there are 10x10 elements. Each grid element represents one hypercolumn of units, capturing representations of object elements.The primary purpose of the V1 layer is to hold the edges detected within a visual input image, each hypercolumn translating to a region of edge-detectors spatially localized (O’Reilly, 2020). Each hypercolumn of the V1 experiences inhibitory competition within the column and with neighboring elements, mirroring the primary visual cortex’s inhibitory competition between localized neurons.

The V4 layer is a grid of 5x5 hypercolumns that receives activation from V1 hypercolumns, and sends activation to the single hypercolumn of units of the IT layer. The IT layer then sends activation to the Output layer. We changed the Output layer to be a single unit that only activates if a face is present in the input. 
The model was trained with uniform batch-processed 100x100 pixel grayscale JPEG images with aligned eyes, noses, and mouths. The faces varied in direction and proportion of the image, but were uniform for facial features on the lines of importance. Images were specifically chosen to have varied race, gender, expression, lighting, and accessories (glasses, jewelry, etc.) to make the model more robust with inferring facial features. 
Training was done with 50 epochs, each epoch having 100 trials. Layer inhibition was set to 1.0. Each epoch had a randomly chosen image for training. As training progressed, the most significant change in the network was the increased activation of units within the V4 layer. With images that shared very similar features over and over, the corresponding units within the V4 layer became more and more activated over time. This allowed for a more accurate recognition of facial features, even with slight variations in image composition. 

![Image 2](https://user-images.githubusercontent.com/89750442/210628205-ff5087ee-fd8a-4a9c-9e2e-0c620285c655.png)

